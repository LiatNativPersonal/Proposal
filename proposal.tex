\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{times}
\usepackage{paralist}
\usepackage[round]{natbib}
\usepackage{color}

\newcommand{\note}[1]{\textit{\small\color{magenta}{#1}}}

\title{Does Proficiency impact the use of cognates in L2?\\[.5em]A Computational Approach}
\author{Liat Nativ \and Advisors: Prof.\ Shuly Wintner and Dr.\ Anat Prior} 
\date{}

\begin{document}
\maketitle
\thispagestyle{empty}

\section{Introduction}

The language of non-native speakers, even highly advanced ones, is different from that of native speakers. One source of such differences is the influence of the speaker's first language (L1) on his or her second language (L2); this is referred to as \emph{transfer effects}. Such transfer is evident, inter alia, in the lexical choices made by non-native speakers. 
%
Our hypothesis in this work is that transfer effects, as reflected by lexical choice, are correlated with the speaker's L2 proficiency. We will employ corpus-based computational methods to investigate this hypothesis.

While the main expected contribution of this work is theoretical, there is also practical motivation for such investigation. By gaining insights into the differences in the language produced by non-native speakers and their correlation with L2 fluency, we can contribute to creating better personalized, L1-based, natural language processing (NLP) solutions. 
A motivation for such personalization can be found, for example, in the work of \cite{heilman-etal-2007-combining},
who suggest that since the process of language 
acquisition inherently differs for first and second-language learners (e.g., grammatical structures are acquired in a much slower rate by second language learners), 
readability assessment should be approached differently for these two populations. 
For similar reasons, the task of \emph{text simplification}, both lexical and grammatical, must be solved differently for native and non-native speakers, most probably taking into account also the speaker's L1. However, to the best of our knowledge, this task is currently addressed uniformly, independently of the target audience.
Summing up, understanding of non-native speakers' lexical choices, combined with the ability to assess their fluency level accordingly,
is fundamental for creating ``personalized'' NLP applications, based on the users' L1 background.

\section{Research Goal}

\emph{Cognates} are words in different languages that have  similar forms and similar meanings due to a common ancestor in some protolanguage. Psycholinguistic research has shown that non-native speakers tend to overuse words that have cognates in their L1.  This tendency has also been established recently on a large scale in a corpus-based computational work \citep{TACL1403}.
Our main goal in the current work is  to show a correlation between a non-native author's choice of cognates and his or her proficiency level in L2 (here, always English). 
We hypothesize that higher proficiency authors will tend to be less influenced by their L1, and therefore use cognates in a manner more similar to that of native speakers.
Usually, this type of study is conducted with a small group of human participants, on which detailed information is collected.  Such psycholinguistic studies are usually focused on a small set of words, and participants are requested to make lexical choices in response to a ready-made questioning \citep{prior:2006a,BIL:8852637}.
We aim to investigate how lexical choices reflected by the use of cognates are correlated to L2 fluency on a larger scale, focusing on dozens of cognates that occur in the spontaneous language of a large group (hundreds) of non-native authors with a variety of L1s.

\section{Previous Work}

Most people in the world are able to express themselves in more than one language \citep{grosjean2012psycholinguistics}, thereby maintaining two or more language systems simultaneously---a task that requires considerable cognitive resources  \citep{schlesinger:2003,hvelplund2014eye,Prior2014,Kroll_Bobb_Hoshino_2014}.
Traces of the mother tongue are likely to be found in the non-native speaker's second language utterances \citep{jarvis2008crosslinguistic}. The differences between native and non-native speakers are so prominent that even highly advanced non-native speakers can be accurately distinguished from natives \citep{tomokiyo2001you,bergsma2012stylometric,DBLP:conf/acl/RabinovichNOW16,D18-1395}. 
Focusing on lexical choices, several authors have shown the preference of bilingual speakers towards cognates, for example in tasks of translation and via eye tracking while reading  \citep{de1992determinants, prior2011translation, libben2009bilingual,cop2017reading}. 
\citet{TACL1403} were able to reconstruct the phylogenetic language tree of the Indo-European language family, based solely on the tendency of non-natives to favor cognates in their native language.

The focus of this work is the correlation between L2 proficiency and cognate facilitation, expecting to observe a lesser transfer effect as the level of proficientcy increases. One motivation for the hypothesis can be found in the work of \cite{Prior-etal:2017}, showing reduced  cross language interference with higher vocabulary.
 Proficiency levels are estimated based on studies dealing with lexical, psycholinguistic and syntactic measures \citep{Kuperman2012,LuAi2015,kyle2015automatically}.

Following the psycholinguistic nature of our hypothesis, most related studies are conducted in an artificial environment, based on a limited number of participants, native languages, and target words. The setting for our work is different: we are using computational analysis to conduct a corpus-based study based on spontaneous (written) language productions by over~1600 speakers and~700 target words. Our work is inspired to a great extent by \citet{TACL1403}.


\section{Research Plan}

\subsection{Dataset}
\label{sec:dataset}

The dataset for this work is based on a large corpus of non-native English: the \emph{L2-Reddit corpus}, released by \citet{TACL1403}. It comprises of social media posts by highly fluent authors who indicate their country as a metadata attribute.  We view the country information as an accurate, albeit not perfect, proxy for the native language of the author \citep{D18-1395}.  
\citet{TACL1403} carefully created  a culture-independent focus set of over~1000 words, forming~541 synonym sets that may reflect cognates in some L1s, but not all of them, and are hence used differently by authors with different linguistic backgrounds.
We further reduced the size of the focus set to about~740 words forming~288 synonym sets, by investigating the etymology of each cognate, leaving only synonym sets that include at least one word of Germanic origin, and at least one of Romance Origin.
The final dataset for this work was extracted from the L2-Reddit corpus and consists of~960 non-native authors whose native language is either Romance or Germanic, who used cognates from the focus set at least~1000 times in their posts.
The non-native group consists of two subgroups of equal size:  authors whose L1 is Germanic and those whose L1 is Romance. 
A control group of approximately the same size of English native authors from 5 different countries (US, UK, New-Zealand, Australia and Ireland), with a similar distribution of cognates from the focus set as the non-native group, was extracted to complete the picture. 

\subsection{English proficiency measures}

We use several commonly accepted measures for assessing the proficiency level of authors. These include both lexical and syntactic measures.

\subsubsection{Lexical and psycholinguistic measures}

Lexical measures include lexical richness, defined as type-token ratio (TTR); average age-of-acquisition (in years) of lexical items \citep{Kuperman2012} and mean word rank, where the rank was retrieved from a list of the entire Reddit dataset vocabulary, sorted by word frequency in the corpus \citep{TACL1403}.  In addition, we also used the psycholinguistic measure of mean naming reaction time (in Milliseconds), based on a list generated from the English Lexicon Project \citep{Balota2007}. These measures were calculated on a random sample of~10000 tokens for each user.


\subsubsection{Syntactic Measures }

Syntactic complexity was assessed through 14 different syntactic measures, using the \emph{L2 Syntactic Complexity Analyzer} \citep{Lu2010-Automatic}: \note{leave only the ones you end up using}

\begin{description}
\item[MLS] mean length of sentence (\# of words/\# of sentences), 
\item[MLT] mean length of T-unit (\# of words/\# of T-units). T-unit is the minimal terminable unit of language that can be considered a grammatical sentence 
\item[MLC] mean length of clause (\# of words/\# of clauses)
\item[C/T] clauses per T-unit (\# of clauses/\# of T-unit)
\item[CT/T] complex T-unit (containing a dependent clause) to all T-units ratio (\# of complex T-units/\# of T-units )
\item[DC/C] dependent clauses per clause
\item[DC/T] dependent clauses per T-unit (\# of dependent clauses/\# of clauses)
\item[CP/C] coordinate phrases per clause (\# of coordinate phrases/\# of clauses )
\item[CP/T] coordinate phrases per T-unit (\# of dependent clauses/\# of T-units)
\item[T/S] T-units per sentence (\# of T-units/\# of sentences)
\item[CN/C] complex nominals per clause (\# of complex nominals/\# of clauses)
\item[CN/T]  complex nominals per T-unit (\# of complex nominals/\# of T-units)
\item[VP/T] verb phrases per T-unit (\# of verb phrases/\# of T-units)
\item[C/S] clauses per sentence (\# of clauses/\# of sentences)
\end{description}

These 14 measures are calculated on a random sample of~1000 sentences for each native and non-native author. 
[This list will be shorter based on the measures weâ€™ll decide that are meaningful]


\subsection{Methodology}

Cognates, by definition - are language-specific. As shown by \citet{TACL1403}, words that have  cognates in the non-natve user's L1 tend to be overrepresented in his or her L2 productions. As explained in the dataset section above, this work focuses on 2 groups of non-native speakrs whose native language's origin is either Romance or Germanic. Following the hypothesis, we expect to see higher frequencies of cognates from Germanic/Romance origin in the Germanic/Romance users group posts respectively, decreasing in correlation with the level of English proficiency, down to the point of frequencies similar to those of native speakrs.
To model the use of cognates and to examine the tendency towards Germanic or Romance cognates, we computed a normalized count for each user and for each synonym set. 
For example, for the synonym set \emph  thankful (from Germanic origin) and \emph grateful (from Romance Origin)   the count for a  user from Germany (Aunvilgod) was 8 for the Germanic word and 3 for the Romance word, then the normalized count will be ~0.727 and ~0.273 for Germanic and Romance cognates, respectively.
For exploring the correlation between the use of cognates and the level of English proficiency, a benchmark of ``good'' English will be calculated based on the lexical selections made by the native authors group. The benchmark is defined as the average of the Germanic/Romance tendency---represented by the normalized count---across all the native authors. This benchmark is calculated for each synonym set independently. 
Once the benchmark is set for a certain synonym set, the lexical choice of each non-native author can be represented by the directed (-/+) distance from that benchmark. The direction is defined arbitrarily to be positive for Germanic and negative for Romance over-representation, respectively.  The deviation from the benchmark as a function of the user proficiency level will provide an indication of the validity of the research hypothesis. 
For example, for the synonym set \emph mindless (from Germanic origin) and \emph senseless (from Romance Origin), native speakers average preference towards \emph mindless (germanic word) is ~0.7098 (calculated ad the average normalized count over all native speakers). This is number is the benchmark from which the non-native speaker deviation will be calculated. And so for a single non-native user from Denmark (dalsgaard) who's germanic normalized count for the same synonym set is ~0.889, the deviation from the threshold is 0.889 -0.7098 = 0.1792. For a different user, from France (CH4F) who used the germanic and romance word equaly - i.e. normalized count = 0.5, the diversion from the threshond will be 0.5 - 0.7098  = -0.2098.



\section{Preliminary Results}

[Draw a few graphs for some of the synsets, based on the selected proficiency measures]


\bibliographystyle{plainnat}
\bibliography{all}

\end{document}

References
